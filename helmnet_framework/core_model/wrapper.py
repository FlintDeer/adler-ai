# wrapper.py


def query_model(prompt: str) -> str:
    """
    Simulates querying the base LLM. Replace this with actual API integration later.
    """
    print(f"\n[DEBUG] Prompt sent to LLM:\n---\n{prompt}\n---\n")
    return "(response placeholder)"